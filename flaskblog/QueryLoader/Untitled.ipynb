{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'\n",
    "    \n",
    "def without_color():\n",
    "    Color.PURPLE = ''\n",
    "    Color.CYAN = ''\n",
    "    Color.DARKCYAN = ''\n",
    "    Color.BLUE = ''\n",
    "    Color.GREEN = ''\n",
    "    Color.YELLOW = ''\n",
    "    Color.RED = ''\n",
    "    Color.BOLD = ''\n",
    "    Color.UNDERLINE = ''\n",
    "    Color.END = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Column:\n",
    "    def __init__(self, name='', type=None, equivalences=None):\n",
    "        self._name = name\n",
    "\n",
    "        if not type:\n",
    "            type = []\n",
    "        self._type = type\n",
    "\n",
    "        if not equivalences:\n",
    "            equivalences = []\n",
    "        self._equivalences = equivalences\n",
    "\n",
    "        self.primary = False\n",
    "        self.foreign = False\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    @property\n",
    "    def type(self):\n",
    "        return self._type\n",
    "\n",
    "    def add_type(self, type):\n",
    "        self.type.append(type)\n",
    "    def get_type(self):\n",
    "        return self.type\n",
    "\n",
    "    @property\n",
    "    def equivalences(self):\n",
    "        return self._equivalences\n",
    "\n",
    "    def add_equivalence(self, equivalence):\n",
    "        self.equivalences.append(equivalence)\n",
    "\n",
    "    def is_equivalent(self, word):\n",
    "        if word in self.equivalences:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_primary(self):\n",
    "        return self.primary\n",
    "\n",
    "    def set_as_primary(self):\n",
    "        self.primary = True\n",
    "\n",
    "    def is_foreign(self):\n",
    "        return self.foreign\n",
    "\n",
    "    def set_as_foreign(self, references):\n",
    "        self.foreign = references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Table:\n",
    "    def __init__(self, name='', columns=None, equivalences=None):\n",
    "        self._name = name\n",
    "\n",
    "        if not columns:\n",
    "            columns = []\n",
    "        self.columns = columns\n",
    "\n",
    "        if not equivalences:\n",
    "            equivalences = []\n",
    "        self.equivalences = equivalences\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    @name.setter\n",
    "    def name(self, value):\n",
    "        self._name = value\n",
    "\n",
    "    def get_number_of_columns(self):\n",
    "        return len(self.columns)\n",
    "\n",
    "    def get_columns(self):\n",
    "        return self.columns\n",
    "\n",
    "    def get_column_by_name(self, column_name):\n",
    "        for column in self.columns:\n",
    "            if column.name == column_name:\n",
    "                return column\n",
    "\n",
    "    def add_column(self, column_name, column_type, column_equivalences):\n",
    "        self.columns.append(Column(column_name, column_type, column_equivalences))\n",
    "\n",
    "    def get_equivalences(self):\n",
    "        return self.equivalences\n",
    "\n",
    "    def add_equivalence(self, equivalence):\n",
    "        self.equivalences.append(equivalence)\n",
    "\n",
    "    def is_equivalent(self, word):\n",
    "        if word in self.equivalences:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_primary_keys(self):\n",
    "        primary_keys = []\n",
    "        for column in self.columns:\n",
    "            if column.is_primary():\n",
    "                primary_keys.append(column)\n",
    "        return primary_keys\n",
    "\n",
    "    def get_primary_key_names(self):\n",
    "        primary_keys = []\n",
    "        for column in self.columns:\n",
    "            if column.is_primary():\n",
    "                primary_keys.append(column.name)\n",
    "        return primary_keys\n",
    "\n",
    "    def add_primary_key(self, primary_key_column):\n",
    "        for column in self.columns:\n",
    "            if column.name == primary_key_column:\n",
    "                column.set_as_primary()\n",
    "\n",
    "    def get_foreign_keys(self):\n",
    "        foreign_keys = []\n",
    "        for column in self.columns:\n",
    "            if column.is_foreign():\n",
    "                foreign_keys.append(column)\n",
    "        return foreign_keys\n",
    "\n",
    "    def get_foreign_key_names(self):\n",
    "        foreign_keys = []\n",
    "        for column in self.columns:\n",
    "            if column.is_foreign():\n",
    "                foreign_keys.append(column.name)\n",
    "        return foreign_keys\n",
    "\n",
    "    def add_foreign_key(self, column_name, foreign_table, foreign_column):\n",
    "        for column in self.columns:\n",
    "            if column.name == column_name:\n",
    "                column.set_as_foreign({'foreign_table': foreign_table, 'foreign_column': foreign_column})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "class Database:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tables = []\n",
    "        self.thesaurus_object = None\n",
    "\n",
    "    def set_thesaurus(self, thesaurus):\n",
    "        self.thesaurus_object = thesaurus\n",
    "\n",
    "    def get_number_of_tables(self):\n",
    "        return len(self.tables)\n",
    "\n",
    "    def get_tables(self):\n",
    "        return self.tables\n",
    "\n",
    "    def get_column_with_this_name(self, name):\n",
    "        for table in self.tables:\n",
    "            for column in table.get_columns():\n",
    "                if column.name == name:\n",
    "                    return column\n",
    "\n",
    "    def get_table_by_name(self, table_name):\n",
    "        for table in self.tables:\n",
    "            if table.name == table_name:\n",
    "                return table\n",
    "\n",
    "    def get_tables_into_dictionary(self):\n",
    "        data = {}\n",
    "        for table in self.tables:\n",
    "            data[table.name] = []\n",
    "            for column in table.get_columns():\n",
    "                data[table.name].append(column.name)\n",
    "        return data\n",
    "\n",
    "    def get_primary_keys_by_table(self):\n",
    "        data = {}\n",
    "        for table in self.tables:\n",
    "            data[table.name] = table.get_primary_keys()\n",
    "        return data\n",
    "\n",
    "    def get_foreign_keys_by_table(self):\n",
    "        data = {}\n",
    "        for table in self.tables:\n",
    "            data[table.name] = table.get_foreign_keys()\n",
    "        return data\n",
    "\n",
    "    def get_primary_keys_of_table(self, table_name):\n",
    "        for table in self.tables:\n",
    "            if table.name == table_name:\n",
    "                return table.get_primary_keys()\n",
    "\n",
    "    def get_primary_key_names_of_table(self, table_name):\n",
    "        for table in self.tables:\n",
    "            if table.name == table_name:\n",
    "                return table.get_primary_key_names()\n",
    "\n",
    "    def get_foreign_keys_of_table(self, table_name):\n",
    "        for table in self.tables:\n",
    "            if table.name == table_name:\n",
    "                return table.get_foreign_keys()\n",
    "\n",
    "    def get_foreign_key_names_of_table(self, table_name):\n",
    "        for table in self.tables:\n",
    "            if table.name == table_name:\n",
    "                return table.get_foreign_key_names()\n",
    "\n",
    "    def add_table(self, table):\n",
    "        self.tables.append(table)\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_path(path):\n",
    "        cwd = os.path.dirname(__file__)\n",
    "        filename = os.path.join(cwd, path)\n",
    "        return filename\n",
    "\n",
    "    def load(self, path):\n",
    "        with open(path) as f:\n",
    "            content = f.read()\n",
    "            tables_string = [p.split(';')[0] for p in content.split('CREATE') if ';' in p]\n",
    "            for table_string in tables_string:\n",
    "                if 'TABLE' in table_string:\n",
    "                    table = self.create_table(table_string)\n",
    "                    self.add_table(table)\n",
    "            alter_tables_string = [p.split(';')[0] for p in content.split('ALTER') if ';' in p]\n",
    "            for alter_table_string in alter_tables_string:\n",
    "                if 'TABLE' in alter_table_string:\n",
    "                    self.alter_table(alter_table_string)\n",
    "\n",
    "    def predict_type(self, string):\n",
    "        if 'int' in string.lower():\n",
    "            return 'int'\n",
    "        elif 'char' in string.lower() or 'text' in string.lower():\n",
    "            return 'string'\n",
    "        elif 'date' in string.lower():\n",
    "            return 'date'\n",
    "        else:\n",
    "            return 'unknow'\n",
    "\n",
    "    def create_table(self, table_string):\n",
    "        lines = table_string.split(\"\\n\")\n",
    "        table = Table()\n",
    "        for line in lines:\n",
    "            if 'TABLE' in line:\n",
    "                table_name = re.search(\"`(\\w+)`\", line)\n",
    "                table.name = table_name.group(1)\n",
    "                if self.thesaurus_object is not None:\n",
    "                    table.equivalences = self.thesaurus_object.get_synonyms_of_a_word(table.name)\n",
    "            elif 'PRIMARY KEY' in line:\n",
    "                primary_key_columns = re.findall(\"`(\\w+)`\", line)\n",
    "                for primary_key_column in primary_key_columns:\n",
    "                    table.add_primary_key(primary_key_column)\n",
    "            else:\n",
    "                column_name = re.search(\"`(\\w+)`\", line)\n",
    "                if column_name is not None:\n",
    "                    column_type = self.predict_type(line)\n",
    "                    if self.thesaurus_object is not None:\n",
    "                        equivalences = self.thesaurus_object.get_synonyms_of_a_word(column_name.group(1))\n",
    "                    else:\n",
    "                        equivalences = []\n",
    "                    table.add_column(column_name.group(1), column_type, equivalences)\n",
    "        return table\n",
    "\n",
    "    def alter_table(self, alter_string):\n",
    "        lines = alter_string.replace('\\n', ' ').split(';')\n",
    "        for line in lines:\n",
    "            if 'PRIMARY KEY' in line:\n",
    "                table_name = re.search(\"TABLE `(\\w+)`\", line).group(1)\n",
    "                table = self.get_table_by_name(table_name)\n",
    "                primary_key_columns = re.findall(\"PRIMARY KEY \\(`(\\w+)`\\)\", line)\n",
    "                for primary_key_column in primary_key_columns:\n",
    "                    table.add_primary_key(primary_key_column)\n",
    "            elif 'FOREIGN KEY' in line:\n",
    "                table_name = re.search(\"TABLE `(\\w+)`\", line).group(1)\n",
    "                table = self.get_table_by_name(table_name)\n",
    "                foreign_keys_list = re.findall(\"FOREIGN KEY \\(`(\\w+)`\\) REFERENCES `(\\w+)` \\(`(\\w+)`\\)\", line)\n",
    "                for column, foreign_table, foreign_column in foreign_keys_list:\n",
    "                    table.add_foreign_key(column, foreign_table, foreign_column)\n",
    "\n",
    "    def print_me(self):\n",
    "        for table in self.tables:\n",
    "            print('+-------------------------------------+')\n",
    "            print(\"| %25s           |\" % (table.name.upper()))\n",
    "            print('+-------------------------------------+')\n",
    "            for column in table.columns:\n",
    "                if column.is_primary():\n",
    "                    print(\"| ðŸ”‘ %31s           |\" % (Color.BOLD + column.name + ' (' + column.get_type() + ')' + Color.END))\n",
    "                elif column.is_foreign():\n",
    "                    print(\"| #ï¸âƒ£ %31s           |\" % (Color.BOLD + column.name + ' (' + column.get_type() + ')' + Color.END))\n",
    "                else:\n",
    "                    print(\"|   %23s           |\" % (column.name + ' (' + column.get_type() + ')'))\n",
    "            print('+-------------------------------------+\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|                      CITY           |\n",
      "+-------------------------------------+\n",
      "| ðŸ”‘                \u001b[1mid (int)\u001b[0m           |\n",
      "|         cityName (string)           |\n",
      "+-------------------------------------+\n",
      "\n",
      "+-------------------------------------+\n",
      "|                       EMP           |\n",
      "+-------------------------------------+\n",
      "| ðŸ”‘                \u001b[1mid (int)\u001b[0m           |\n",
      "|             name (string)           |\n",
      "| #ï¸âƒ£            \u001b[1mcityId (int)\u001b[0m           |\n",
      "|               score (int)           |\n",
      "+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "database = Database()\n",
    "database.load('database_store/city.sql')\n",
    "database.print_me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Select():\n",
    "    def __init__(self):\n",
    "        self.columns = []\n",
    "\n",
    "    def add_column(self, column, column_type):\n",
    "        if [column, column_type] not in self.columns:\n",
    "            self.columns.append([column, column_type])\n",
    "\n",
    "    def get_columns(self):\n",
    "        return self.columns\n",
    "\n",
    "    def get_just_column_name(self, column):\n",
    "        if column != str(None):\n",
    "            return column.rsplit('.', 1)[1]\n",
    "        else:\n",
    "            return column\n",
    "\n",
    "    def print_column(self, selection):\n",
    "        column = selection[0]\n",
    "        column_type = selection[1]\n",
    "\n",
    "        if column is None:\n",
    "            if column_type is not None:\n",
    "                if 'COUNT' in column_type:\n",
    "                    return Color.BOLD + 'COUNT(' + Color.END + '*' + Color.BOLD + ')' + Color.END\n",
    "                else:\n",
    "                    return '*'\n",
    "            else:\n",
    "                return '*'\n",
    "        else:\n",
    "            if 'DISTINCT' in column_type:\n",
    "                if 'COUNT' in column_type:\n",
    "                    return Color.BOLD + 'COUNT(DISTINCT ' + Color.END + str(column) + Color.BOLD + ')' + Color.END\n",
    "                else:\n",
    "                    return Color.BOLD + 'DISTINCT ' + Color.END + str(column)\n",
    "            if 'COUNT' in column_type:\n",
    "                return Color.BOLD + 'COUNT(' + Color.END + str(column) + Color.BOLD + ')' + Color.END\n",
    "            elif 'AVG' in column_type:\n",
    "                return Color.BOLD + 'AVG(' + Color.END + str(column) + Color.BOLD + ')' + Color.END\n",
    "            elif 'SUM' in column_type:\n",
    "                return Color.BOLD + 'SUM(' + Color.END + str(column) + Color.BOLD + ')' + Color.END\n",
    "            elif 'MAX' in column_type:\n",
    "                return Color.BOLD + 'MAX(' + Color.END + str(column) + Color.BOLD + ')' + Color.END\n",
    "            elif 'MIN' in column_type:\n",
    "                return Color.BOLD + 'MIN(' + Color.END + str(column) + Color.BOLD + ')' + Color.END\n",
    "            else:\n",
    "                return str(column)\n",
    "\n",
    "    def __str__(self):\n",
    "        select_string = ''\n",
    "        for i in range(0, len(self.columns)):\n",
    "            if i == (len(self.columns) - 1):\n",
    "                select_string = select_string + str(self.print_column(self.columns[i]))\n",
    "            else:\n",
    "                select_string = select_string + str(self.print_column(self.columns[i])) + ', '\n",
    "\n",
    "        return Color.BOLD + 'SELECT ' + Color.END + select_string\n",
    "\n",
    "    def print_json(self, output):\n",
    "        if len(self.columns) >= 1:\n",
    "            if len(self.columns) == 1:\n",
    "                output.write('\\t\"select\": {\\n')\n",
    "                output.write('\\t\\t\"column\": \"' + self.get_just_column_name(str(self.columns[0][0])) + '\",\\n')\n",
    "                output.write('\\t\\t\"type\": \"' + str(self.columns[0][1]) + '\"\\n')\n",
    "                output.write('\\t},\\n')\n",
    "            else:\n",
    "                output.write('\\t\"select\": {\\n')\n",
    "                output.write('\\t\\t\"columns\": [\\n')\n",
    "                for i in range(0, len(self.columns)):\n",
    "                    if i == (len(self.columns) - 1):\n",
    "                        output.write(\n",
    "                            '\\t\\t\\t{ \"column\": \"' + self.get_just_column_name(str(self.columns[i][0])) + '\",\\n')\n",
    "                        output.write('\\t\\t\\t  \"type\": \"' + str(self.columns[i][1]) + '\"\\n')\n",
    "                        output.write('\\t\\t\\t}\\n')\n",
    "                    else:\n",
    "                        output.write(\n",
    "                            '\\t\\t\\t{ \"column\": \"' + self.get_just_column_name(str(self.columns[i][0])) + '\",\\n')\n",
    "                        output.write('\\t\\t\\t  \"type\": \"' + str(self.columns[i][1]) + '\"\\n')\n",
    "                        output.write('\\t\\t\\t},\\n')\n",
    "                output.write('\\t\\t]\\n')\n",
    "                output.write('\\t},\\n')\n",
    "        else:\n",
    "            output.write('\\t\"select\": {\\n')\n",
    "            output.write('\\t},\\n')\n",
    "\n",
    "\n",
    "class From():\n",
    "    table = ''\n",
    "\n",
    "    def __init__(self, table=None):\n",
    "        if table is not None:\n",
    "            self.table = table\n",
    "        else:\n",
    "            self.table = ''\n",
    "\n",
    "    def set_table(self, table):\n",
    "        self.table = table\n",
    "\n",
    "    def get_table(self):\n",
    "        return self.table\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n' + Color.BOLD + 'FROM ' + Color.END + str(self.table)\n",
    "\n",
    "    def print_json(self, output):\n",
    "        if self.table != '':\n",
    "            output.write('\\t\"from\": {\\n')\n",
    "            output.write('\\t\\t\"table\": \"' + str(self.table) + '\"\\n')\n",
    "            output.write('\\t},\\n')\n",
    "        else:\n",
    "            output.write('\\t\"from\": {\\n')\n",
    "            output.write('\\t},\\n')\n",
    "\n",
    "\n",
    "\n",
    "class Condition():\n",
    "    column = ''\n",
    "    column_type = ''\n",
    "    operator = ''\n",
    "    value = ''\n",
    "\n",
    "    def __init__(self, column, column_type, operator, value):\n",
    "        self.column = column\n",
    "        self.column_type = column_type\n",
    "        self.operator = operator\n",
    "        self.value = value\n",
    "\n",
    "    def get_column(self):\n",
    "        return self.column\n",
    "\n",
    "    def get_column_type(self):\n",
    "        return self.column_type\n",
    "\n",
    "    def get_operator(self):\n",
    "        return self.operator\n",
    "\n",
    "    def get_value(self):\n",
    "        return self.value\n",
    "\n",
    "    def get_in_list(self):\n",
    "        return [self.column, self.column_type, self.operator, self.value]\n",
    "\n",
    "    def get_just_column_name(self, column):\n",
    "        if column != str(None):\n",
    "            return column.rsplit('.', 1)[1]\n",
    "        else:\n",
    "            return column\n",
    "\n",
    "    def get_column_with_type_operation(self, column, column_type):\n",
    "        if column_type is None:\n",
    "            return self.column\n",
    "        else:\n",
    "            return Color.BOLD + str(column_type) + '(' + Color.END + self.column + Color.BOLD + ')' + Color.END\n",
    "\n",
    "    def get_pretty_operator(self, operator):\n",
    "        if operator == 'BETWEEN':\n",
    "            return Color.BOLD + 'BETWEEN' + Color.END + ' OOV ' + Color.BOLD + 'AND' + Color.END\n",
    "        else:\n",
    "            return Color.BOLD + operator + Color.END\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.get_column_with_type_operation(self.column, self.column_type)) + ' ' + str(\n",
    "            self.get_pretty_operator(self.operator)) + ' ' + str(self.value)\n",
    "\n",
    "    def print_json(self, output):\n",
    "\n",
    "        output.write(\n",
    "            '\\t\\t\\t{ \"column\": \"' + self.get_just_column_name(str(self.column)) + '\",\\n\\t\\t\\t  \"type\": \"' + str(\n",
    "                self.column_type) + '\",\\n\\t\\t\\t  \"operator\": \"' + str(self.operator) + '\",\\n\\t\\t\\t  \"value\": \"' + str(\n",
    "                self.value) + '\"\\n\\t\\t\\t}')\n",
    "\n",
    "\n",
    "class Where():\n",
    "    conditions = []\n",
    "\n",
    "    def __init__(self, clause=None):\n",
    "        if clause is not None:\n",
    "            self.conditions.append([None, clause])\n",
    "        else:\n",
    "            self.conditions = []\n",
    "\n",
    "    def add_condition(self, junction, clause):\n",
    "        self.conditions.append([junction, clause])\n",
    "\n",
    "    def get_conditions(self):\n",
    "        return self.conditions\n",
    "\n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "\n",
    "        if len(self.conditions) >= 1:\n",
    "            for i in range(0, len(self.conditions)):\n",
    "                if i == 0:\n",
    "                    string += '\\n' + Color.BOLD + 'WHERE' + Color.END + ' ' + str(self.conditions[i][1])\n",
    "                else:\n",
    "                    string += '\\n' + Color.BOLD + str(self.conditions[i][0]) + Color.END + ' ' + str(\n",
    "                        self.conditions[i][1])\n",
    "\n",
    "            return string\n",
    "        else:\n",
    "            return string\n",
    "\n",
    "    def print_json(self, output):\n",
    "        if len(self.conditions) >= 1:\n",
    "            if len(self.conditions) == 1:\n",
    "                output.write('\\t\"where\": {\\n')\n",
    "                output.write('\\t\\t\"condition\": [\\n')\n",
    "                self.conditions[0][1].print_json(output)\n",
    "\n",
    "                output.write('\\n')\n",
    "                output.write('\\t\\t]\\n')\n",
    "                output.write('\\t},\\n')\n",
    "            else:\n",
    "                output.write('\\t\"where\": {\\n')\n",
    "                output.write('\\t\\t\"conditions\": [\\n')\n",
    "                for i in range(0, len(self.conditions)):\n",
    "                    if i != 0:\n",
    "                        output.write('\\t\\t\\t{\\n\\t\\t\\t  \"operator\": \"' + str(self.conditions[i][0]) + '\"\\n\\t\\t\\t},\\n')\n",
    "                    self.conditions[i][1].print_json(output)\n",
    "                    if i != (len(self.conditions) - 1):\n",
    "                        output.write(',')\n",
    "                    output.write('\\n')\n",
    "                output.write('\\t\\t]\\n')\n",
    "                output.write('\\t},\\n')\n",
    "        else:\n",
    "            output.write('\\t\"where\": {\\n')\n",
    "            output.write('\\t},\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Query():\n",
    "    select = None\n",
    "    _from = None\n",
    "    join = None\n",
    "    where = None\n",
    "    group_by = None\n",
    "    order_by = None\n",
    "\n",
    "    def __init__(self, select=None, _from=None, join=None, where=None, group_by=None, order_by=None):\n",
    "        if select is not None:\n",
    "            self.select = select\n",
    "        else:\n",
    "            self.select = None\n",
    "        if _from is not None:\n",
    "            self._from = _from\n",
    "        else:\n",
    "            self._from = None\n",
    "        if join is not None:\n",
    "            self.join = join\n",
    "        else:\n",
    "            self.join = None\n",
    "        if where is not None:\n",
    "            self.where = where\n",
    "        else:\n",
    "            self.where = None\n",
    "        if group_by is not None:\n",
    "            self.group_by = group_by\n",
    "        else:\n",
    "            self.group_by = None\n",
    "        if order_by is not None:\n",
    "            self.order_by = order_by\n",
    "        else:\n",
    "            self.order_by = None\n",
    "\n",
    "    def set_select(self, select):\n",
    "        self.select = select\n",
    "\n",
    "    def get_select(self):\n",
    "        return self.select\n",
    "\n",
    "    def set_from(self, _from):\n",
    "        self._from = _from\n",
    "\n",
    "    def get_from(self):\n",
    "        return self._from\n",
    "\n",
    "    def set_join(self, join):\n",
    "        self.join = join\n",
    "\n",
    "    def get_join(self):\n",
    "        return self.join\n",
    "\n",
    "    def set_where(self, where):\n",
    "        self.where = where\n",
    "\n",
    "    def get_where(self):\n",
    "        return self.where\n",
    "\n",
    "    def set_group_by(self, group_by):\n",
    "        self.group_by = group_by\n",
    "\n",
    "    def get_group_by(self):\n",
    "        return self.group_by\n",
    "\n",
    "    def set_order_by(self, order_by):\n",
    "        self.order_by = order_by\n",
    "\n",
    "    def get_order_by(self):\n",
    "        return self.order_by\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n' + str(self.select) + str(self._from) + str(self.join) + str(self.where) + str(self.group_by) + str(\n",
    "            self.order_by) + ';\\n'\n",
    "\n",
    "    def print_json(self, filename=\"output.json\"):\n",
    "        output = open(filename, 'a')\n",
    "        output.write('{\\n')\n",
    "        self.select.print_json(output)\n",
    "        self._from.print_json(output)\n",
    "        self.join.print_json(output)\n",
    "        self.where.print_json(output)\n",
    "        self.group_by.print_json(output)\n",
    "        self.order_by.print_json(output)\n",
    "        output.write('}\\n')\n",
    "        output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParsingException(Exception):\n",
    "    def __init__(self, reason=''):\n",
    "        self.reason = reason\n",
    "\n",
    "    def __str__(self):\n",
    "        return Color.BOLD + Color.RED + self.reason + Color.END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import sys\n",
    "import unicodedata\n",
    "import functools\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "class SelectParser(Thread):\n",
    "    def __init__(self, columns_of_select, tables_of_from, phrase, count_keywords, sum_keywords, average_keywords,\n",
    "                 max_keywords, min_keywords, distinct_keywords, database_dico, database_object):\n",
    "        Thread.__init__(self)\n",
    "        self.select_objects = []\n",
    "        self.columns_of_select = columns_of_select\n",
    "        self.tables_of_from = tables_of_from\n",
    "        self.phrase = phrase\n",
    "        self.count_keywords = count_keywords\n",
    "        self.sum_keywords = sum_keywords\n",
    "        self.average_keywords = average_keywords\n",
    "        self.max_keywords = max_keywords\n",
    "        self.min_keywords = min_keywords\n",
    "        self.distinct_keywords = distinct_keywords\n",
    "        self.database_dico = database_dico\n",
    "        self.database_object = database_object\n",
    "\n",
    "    def get_tables_of_column(self, column):\n",
    "        tmp_table = []\n",
    "        for table in self.database_dico:\n",
    "            if column in self.database_dico[table]:\n",
    "                tmp_table.append(table)\n",
    "        return tmp_table\n",
    "\n",
    "    def get_column_name_with_alias_table(self, column, table_of_from):\n",
    "        one_table_of_column = self.get_tables_of_column(column)[0]\n",
    "        tables_of_column = self.get_tables_of_column(column)\n",
    "        if table_of_from in tables_of_column:\n",
    "            return str(table_of_from) + '.' + str(column)\n",
    "        else:\n",
    "            return str(one_table_of_column) + '.' + str(column)\n",
    "\n",
    "    def uniquify(self, list):\n",
    "        already = []\n",
    "        for element in list:\n",
    "            if element not in already:\n",
    "                already.append(element)\n",
    "        return already\n",
    "\n",
    "    def run(self):\n",
    "        for table_of_from in self.tables_of_from:  # for each query\n",
    "            self.select_object = Select()\n",
    "            is_count = False\n",
    "            self.columns_of_select = self.uniquify(self.columns_of_select)\n",
    "            number_of_select_column = len(self.columns_of_select)\n",
    "\n",
    "            if number_of_select_column == 0:\n",
    "                select_type = []\n",
    "                for count_keyword in self.count_keywords:\n",
    "                    # if count_keyword in (word.lower() for word in self.phrase):\n",
    "                    # so that it matches multiple words too in keyword synonymn in .lang rather than just single word for COUNT\n",
    "                    # (e.g. QUERY-> \"how many city there are in which the employe name is aman ?\" )\n",
    "                    lower_self_phrase = ' '.join(word.lower() for word in self.phrase)\n",
    "                    if count_keyword in lower_self_phrase:\n",
    "                        select_type.append('COUNT')\n",
    "\n",
    "                self.select_object.add_column(None, self.uniquify(select_type))\n",
    "            else:\n",
    "                select_phrases = []\n",
    "                previous_index = 0\n",
    "\n",
    "                for i in range(0, len(self.phrase)):\n",
    "                    for column_name in self.columns_of_select:\n",
    "                        if (self.phrase[i] == column_name) or (\n",
    "                                    self.phrase[i] in self.database_object.get_column_with_this_name(column_name).equivalences):\n",
    "                            select_phrases.append(self.phrase[previous_index:i + 1])\n",
    "                            previous_index = i + 1\n",
    "\n",
    "                select_phrases.append(self.phrase[previous_index:])\n",
    "\n",
    "                for i in range(0, len(select_phrases)):  # for each select phrase (i.e. column processing)\n",
    "                    select_type = []\n",
    "\n",
    "                    phrase = [word.lower() for word in select_phrases[i]]\n",
    "\n",
    "                    for keyword in self.average_keywords:\n",
    "                        if keyword in phrase:\n",
    "                            select_type.append('AVG')\n",
    "                    for keyword in self.count_keywords:\n",
    "                        if keyword in phrase:\n",
    "                            select_type.append('COUNT')\n",
    "                    for keyword in self.max_keywords:\n",
    "                        if keyword in phrase:\n",
    "                            select_type.append('MAX')\n",
    "                    for keyword in self.min_keywords:\n",
    "                        if keyword in phrase:\n",
    "                            select_type.append('MIN')\n",
    "                    for keyword in self.sum_keywords:\n",
    "                        if keyword in phrase:\n",
    "                            select_type.append('SUM')\n",
    "                    for keyword in self.distinct_keywords:\n",
    "                        if keyword in phrase:\n",
    "                            select_type.append('DISTINCT')\n",
    "\n",
    "                    if (i != len(select_phrases) - 1):\n",
    "                        column = self.get_column_name_with_alias_table(self.columns_of_select[i], table_of_from)\n",
    "                        self.select_object.add_column(column, self.uniquify(select_type))\n",
    "\n",
    "            self.select_objects.append(self.select_object)\n",
    "\n",
    "    def join(self):\n",
    "        Thread.join(self)\n",
    "        return self.select_objects\n",
    "\n",
    "\n",
    "class FromParser(Thread):\n",
    "    def __init__(self, tables_of_from, columns_of_select, columns_of_where, database_object):\n",
    "        Thread.__init__(self)\n",
    "        self.queries = []\n",
    "        self.tables_of_from = tables_of_from\n",
    "        self.columns_of_select = columns_of_select\n",
    "        self.columns_of_where = columns_of_where\n",
    "\n",
    "        self.database_object = database_object\n",
    "        self.database_dico = self.database_object.get_tables_into_dictionary()\n",
    "\n",
    "    def get_tables_of_column(self, column):\n",
    "        tmp_table = []\n",
    "        for table in self.database_dico:\n",
    "            if column in self.database_dico[table]:\n",
    "                tmp_table.append(table)\n",
    "        return tmp_table\n",
    "\n",
    "    def intersect(self, a, b):\n",
    "        return list(set(a) & set(b))\n",
    "\n",
    "    def difference(self, a, b):\n",
    "        differences = []\n",
    "        for _list in a:\n",
    "            if _list not in b:\n",
    "                differences.append(_list)\n",
    "        return differences\n",
    "\n",
    "    def is_direct_join_is_possible(self, table_src, table_trg):\n",
    "        fk_column_of_src_table = self.database_object.get_foreign_keys_of_table(table_src)\n",
    "        fk_column_of_trg_table = self.database_object.get_foreign_keys_of_table(table_trg)\n",
    "\n",
    "        for column in fk_column_of_src_table:\n",
    "            if column.is_foreign()['foreign_table'] == table_trg:\n",
    "                return [(table_src, column.name), (table_trg, column.is_foreign()['foreign_column'])]\n",
    "\n",
    "        for column in fk_column_of_trg_table:\n",
    "            if column.is_foreign()['foreign_table'] == table_src:\n",
    "                return [(table_src, column.is_foreign()['foreign_column']), (table_trg, column.name)]\n",
    "\n",
    "                # pk_table_src = self.database_object.get_primary_key_names_of_table(table_src)\n",
    "                # pk_table_trg = self.database_object.get_primary_key_names_of_table(table_trg)\n",
    "                # match_pk_table_src_with_table_trg = self.intersect(pk_table_src, self.database_dico[table_trg])\n",
    "                # match_pk_table_trg_with_table_src = self.intersect(pk_table_trg, self.database_dico[table_src])\n",
    "\n",
    "                # if len(match_pk_table_src_with_table_trg) >= 1:\n",
    "                #     return [(table_trg, match_pk_table_src_with_table_trg[0]), (table_src, match_pk_table_src_with_table_trg[0])]\n",
    "                # elif len(match_pk_table_trg_with_table_src) >= 1:\n",
    "                # return [(table_trg, match_pk_table_trg_with_table_src[0]),\n",
    "                # (table_src, match_pk_table_trg_with_table_src[0])]\n",
    "\n",
    "    def get_all_direct_linked_tables_of_a_table(self, table_src):\n",
    "        links = []\n",
    "        for table_trg in self.database_dico:\n",
    "            if table_trg != table_src:\n",
    "                link = self.is_direct_join_is_possible(table_src, table_trg)\n",
    "                if link is not None:\n",
    "                    links.append(link)\n",
    "        return links\n",
    "\n",
    "    def is_join(self, historic, table_src, table_trg):\n",
    "        historic = historic\n",
    "        links = self.get_all_direct_linked_tables_of_a_table(table_src)\n",
    "\n",
    "        differences = []\n",
    "        for join in links:\n",
    "            if join[0][0] not in historic:\n",
    "                differences.append(join)\n",
    "        links = differences\n",
    "\n",
    "        for join in links:\n",
    "            if join[1][0] == table_trg:\n",
    "                return [0, join]\n",
    "\n",
    "        path = []\n",
    "        historic.append(table_src)\n",
    "\n",
    "        for join in links:\n",
    "            result = [1, self.is_join(historic, join[1][0], table_trg)]\n",
    "            if result[1] != []:\n",
    "                if result[0] == 0:\n",
    "                    path.append(result[1])\n",
    "                    path.append(join)\n",
    "                else:\n",
    "                    path = result[1]\n",
    "                    path.append(join)\n",
    "        return path\n",
    "\n",
    "    def get_link(self, table_src, table_trg):\n",
    "        path = self.is_join([], table_src, table_trg)\n",
    "        if len(path) > 0:\n",
    "            path.pop(0)\n",
    "            path.reverse()\n",
    "        return path\n",
    "\n",
    "    def unique(self, _list):\n",
    "        return [list(x) for x in set(tuple(x) for x in _list)]\n",
    "\n",
    "    def unique_ordered(self, _list):\n",
    "        frequency = []\n",
    "        for element in _list:\n",
    "            if element not in frequency:\n",
    "                frequency.append(element)\n",
    "        return frequency\n",
    "\n",
    "    def run(self):\n",
    "        self.queries = []\n",
    "\n",
    "        for table_of_from in self.tables_of_from:\n",
    "            links = []\n",
    "            query = Query()\n",
    "            query.set_from(From(table_of_from))\n",
    "            join_object = Join()\n",
    "\n",
    "            for column in self.columns_of_select:\n",
    "                if column not in self.database_dico[table_of_from]:\n",
    "                    foreign_table = self.get_tables_of_column(column)[0]\n",
    "                    join_object.add_table(foreign_table)\n",
    "                    link = self.get_link(table_of_from, foreign_table)\n",
    "\n",
    "                    if not link:\n",
    "                        self.queries = ParsingException(\n",
    "                            \"There is at least column `\" + column + \"` that is unreachable from table `\" + table_of_from.upper() + \"`!\")\n",
    "                        return\n",
    "                    else:\n",
    "                        links.extend(link)\n",
    "\n",
    "            for column in self.columns_of_where:\n",
    "                if column not in self.database_dico[table_of_from]:\n",
    "                    foreign_table = self.get_tables_of_column(column)[0]\n",
    "                    join_object.add_table(foreign_table)\n",
    "                    link = self.get_link(table_of_from, foreign_table)\n",
    "\n",
    "                    if not link:\n",
    "                        self.queries = ParsingException(\n",
    "                            \"There is at least column `\" + column + \"` that is unreachable from table `\" + table_of_from.upper() + \"`!\")\n",
    "                        return\n",
    "                    else:\n",
    "                        links.extend(link)\n",
    "\n",
    "            join_object.set_links(self.unique_ordered(links))\n",
    "            query.set_join(join_object)\n",
    "            self.queries.append(query)\n",
    "\n",
    "    def join(self):\n",
    "        Thread.join(self)\n",
    "        return self.queries\n",
    "\n",
    "\n",
    "class WhereParser(Thread):\n",
    "    def __init__(self, phrases, tables_of_from, columns_of_values_of_where, count_keywords, sum_keywords,\n",
    "                 average_keywords, max_keywords, min_keywords, greater_keywords, less_keywords, between_keywords,\n",
    "                 negation_keywords, junction_keywords, disjunction_keywords, like_keywords, distinct_keywords,\n",
    "                 database_dico, database_object):\n",
    "        Thread.__init__(self)\n",
    "        self.where_objects = []\n",
    "        self.phrases = phrases\n",
    "        self.tables_of_from = tables_of_from\n",
    "        self.columns_of_values_of_where = columns_of_values_of_where\n",
    "        self.count_keywords = count_keywords\n",
    "        self.sum_keywords = sum_keywords\n",
    "        self.average_keywords = average_keywords\n",
    "        self.max_keywords = max_keywords\n",
    "        self.min_keywords = min_keywords\n",
    "        self.greater_keywords = greater_keywords\n",
    "        self.less_keywords = less_keywords\n",
    "        self.between_keywords = between_keywords\n",
    "        self.negation_keywords = negation_keywords\n",
    "        self.junction_keywords = junction_keywords\n",
    "        self.disjunction_keywords = disjunction_keywords\n",
    "        self.like_keywords = like_keywords\n",
    "        self.distinct_keywords = distinct_keywords\n",
    "        self.database_dico = database_dico\n",
    "        self.database_object = database_object\n",
    "\n",
    "    def get_tables_of_column(self, column):\n",
    "        tmp_table = []\n",
    "        for table in self.database_dico:\n",
    "            if column in self.database_dico[table]:\n",
    "                tmp_table.append(table)\n",
    "        return tmp_table\n",
    "\n",
    "    def get_column_name_with_alias_table(self, column, table_of_from):\n",
    "        one_table_of_column = self.get_tables_of_column(column)[0]\n",
    "        tables_of_column = self.get_tables_of_column(column)\n",
    "        if table_of_from in tables_of_column:\n",
    "            return str(table_of_from) + '.' + str(column)\n",
    "        else:\n",
    "            return str(one_table_of_column) + '.' + str(column)\n",
    "\n",
    "    def intersect(self, a, b):\n",
    "        return list(set(a) & set(b))\n",
    "\n",
    "    def predict_operation_type(self, previous_column_offset, current_column_offset):\n",
    "        interval_offset = list(range(previous_column_offset, current_column_offset))\n",
    "        if (len(self.intersect(interval_offset, self.count_keyword_offset)) >= 1):\n",
    "            return 'COUNT'\n",
    "        elif (len(self.intersect(interval_offset, self.sum_keyword_offset)) >= 1):\n",
    "            return 'SUM'\n",
    "        elif (len(self.intersect(interval_offset, self.average_keyword_offset)) >= 1):\n",
    "            return 'AVG'\n",
    "        elif (len(self.intersect(interval_offset, self.max_keyword_offset)) >= 1):\n",
    "            return 'MAX'\n",
    "        elif (len(self.intersect(interval_offset, self.min_keyword_offset)) >= 1):\n",
    "            return 'MIN'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def predict_operator(self, current_column_offset, next_column_offset):\n",
    "        interval_offset = list(range(current_column_offset, next_column_offset))\n",
    "\n",
    "        if (len(self.intersect(interval_offset, self.negation_keyword_offset)) >= 1) and (\n",
    "                    len(self.intersect(interval_offset, self.greater_keyword_offset)) >= 1):\n",
    "            return '<'\n",
    "        elif (len(self.intersect(interval_offset, self.negation_keyword_offset)) >= 1) and (\n",
    "                    len(self.intersect(interval_offset, self.less_keyword_offset)) >= 1):\n",
    "            return '>'\n",
    "        if (len(self.intersect(interval_offset, self.less_keyword_offset)) >= 1):\n",
    "            return '<'\n",
    "        elif (len(self.intersect(interval_offset, self.greater_keyword_offset)) >= 1):\n",
    "            return '>'\n",
    "        elif (len(self.intersect(interval_offset, self.between_keyword_offset)) >= 1):\n",
    "            return 'BETWEEN'\n",
    "        elif (len(self.intersect(interval_offset, self.negation_keyword_offset)) >= 1):\n",
    "            return '!='\n",
    "        elif (len(self.intersect(interval_offset, self.like_keyword_offset)) >= 1):\n",
    "            return 'LIKE'\n",
    "        else:\n",
    "            return '='\n",
    "\n",
    "    def predict_junction(self, previous_column_offset, current_column_offset):\n",
    "        interval_offset = list(range(previous_column_offset, current_column_offset))\n",
    "        junction = 'AND'\n",
    "        if (len(self.intersect(interval_offset, self.disjunction_keyword_offset)) >= 1):\n",
    "            return 'OR'\n",
    "        elif (len(self.intersect(interval_offset, self.junction_keyword_offset)) >= 1):\n",
    "            return 'AND'\n",
    "\n",
    "        first_encountered_junction_offset = -1\n",
    "        first_encountered_disjunction_offset = -1\n",
    "\n",
    "        for offset in self.junction_keyword_offset:\n",
    "            if offset >= current_column_offset:\n",
    "                first_encountered_junction_offset = offset\n",
    "                break\n",
    "\n",
    "        for offset in self.disjunction_keyword_offset:\n",
    "            if offset >= current_column_offset:\n",
    "                first_encountered_disjunction_offset = offset\n",
    "                break\n",
    "\n",
    "        if first_encountered_junction_offset >= first_encountered_disjunction_offset:\n",
    "            return 'AND'\n",
    "        else:\n",
    "            return 'OR'\n",
    "\n",
    "    def uniquify(self, list):\n",
    "        already = []\n",
    "        for element in list:\n",
    "            if element not in already:\n",
    "                already.append(element)\n",
    "        return already\n",
    "\n",
    "    def run(self):\n",
    "        number_of_where_columns = 0\n",
    "        columns_of_where = []\n",
    "        offset_of = {}\n",
    "        column_offset = []\n",
    "        self.count_keyword_offset = []\n",
    "        self.sum_keyword_offset = []\n",
    "        self.average_keyword_offset = []\n",
    "        self.max_keyword_offset = []\n",
    "        self.min_keyword_offset = []\n",
    "        self.greater_keyword_offset = []\n",
    "        self.less_keyword_offset = []\n",
    "        self.between_keyword_offset = []\n",
    "        self.junction_keyword_offset = []\n",
    "        self.disjunction_keyword_offset = []\n",
    "        self.negation_keyword_offset = []\n",
    "        self.like_keyword_offset = []\n",
    "\n",
    "        for phrase in self.phrases:\n",
    "            phrase_offset_string = ''\n",
    "            for i in range(0, len(phrase)):\n",
    "                for table_name in self.database_dico:\n",
    "                    columns = self.database_object.get_table_by_name(table_name).get_columns()\n",
    "                    for column in columns:\n",
    "                        if (phrase[i] == column.name) or (phrase[i] in column.equivalences):\n",
    "                            number_of_where_columns += 1\n",
    "                            columns_of_where.append(column.name)\n",
    "                            offset_of[phrase[i]] = i\n",
    "                            column_offset.append(i)\n",
    "                            break\n",
    "                    else:\n",
    "                        continue\n",
    "                    break\n",
    "\n",
    "                phrase_keyword = str(phrase[i]).lower()  # for robust keyword matching\n",
    "                phrase_offset_string += phrase_keyword + \" \"\n",
    "\n",
    "                for keyword in self.count_keywords:\n",
    "                    if keyword in phrase_offset_string :    # before the column\n",
    "                        if (phrase_offset_string.find(keyword) + len(keyword) + 1 == len(phrase_offset_string) ) :\n",
    "                            self.count_keyword_offset.append(i)\n",
    "\n",
    "                for keyword in self.sum_keywords:\n",
    "                    if keyword in phrase_offset_string :    # before the column\n",
    "                        if (phrase_offset_string.find(keyword) + len(keyword) + 1 == len(phrase_offset_string) ) :\n",
    "                            self.sum_keyword_offset.append(i)\n",
    "\n",
    "                for keyword in self.average_keywords:\n",
    "                    if keyword in phrase_offset_string :    # before the column\n",
    "                        if (phrase_offset_string.find(keyword) + len(keyword) + 1 == len(phrase_offset_string) ) :\n",
    "                            self.average_keyword_offset.append(i)\n",
    "\n",
    "                for keyword in self.max_keywords:\n",
    "                    if keyword in phrase_offset_string :    # before the column\n",
    "                        if (phrase_offset_string.find(keyword) + len(keyword) + 1 == len(phrase_offset_string) ) :\n",
    "                            self.max_keyword_offset.append(i)\n",
    "\n",
    "                for keyword in self.min_keywords:\n",
    "                    if keyword in phrase_offset_string :    # before the column\n",
    "                        if (phrase_offset_string.find(keyword) + len(keyword) + 1 == len(phrase_offset_string) ) :\n",
    "                            self.min_keyword_offset.append(i)\n",
    "\n",
    "                for keyword in self.greater_keywords:\n",
    "                    if keyword in phrase_offset_string :    # after the column\n",
    "                        if (phrase_offset_string.find(keyword) + len(keyword) + 1 == len(phrase_offset_string) ) :\n",
    "                            self.greater_keyword_offset.append(i)\n",
    "\n",
    "                for keyword in self.less_keywords:\n",
    "                    if keyword in phrase_offset_string :    # after the column\n",
    "                        if (phrase_offset_string.find(keyword) + len(keyword) + 1 == len(phrase_offset_string) ) :\n",
    "                            self.less_keyword_offset.append(i)\n",
    "\n",
    "                for keyword in self.between_keywords:\n",
    "                    if keyword in phrase_offset_string :    # after the column\n",
    "                        if (phrase_offset_string.find(keyword) + len(keyword) + 1 == len(phrase_offset_string) ) :\n",
    "                            self.between_keyword_offset.append(i)\n",
    "\n",
    "                for keyword in self.junction_keywords:\n",
    "                    if keyword in phrase_offset_string :    # after the column\n",
    "                        if (phrase_offset_string.find(keyword) + len(keyword) + 1 == len(phrase_offset_string) ) :\n",
    "                            self.junction_keyword_offset.append(i)\n",
    "\n",
    "                for keyword in self.disjunction_keywords:\n",
    "                    if keyword in phrase_offset_string :    # after the column\n",
    "                        if (phrase_offset_string.find(keyword) + len(keyword) + 1 == len(phrase_offset_string) ) :\n",
    "                            self.disjunction_keyword_offset.append(i)\n",
    "\n",
    "                for keyword in self.negation_keywords:\n",
    "                    if keyword in phrase_offset_string :\n",
    "                        if (phrase_offset_string.find(keyword) + len(keyword) + 1 == len(phrase_offset_string) ) :\n",
    "                            self.negation_keyword_offset.append(i)\n",
    "\n",
    "                for keyword in self.like_keywords:\n",
    "                    if keyword in phrase_offset_string :    # after the column\n",
    "                        if (phrase_offset_string.find(keyword) + len(keyword) + 1 == len(phrase_offset_string) ) :\n",
    "                            self.like_keyword_offset.append(i)\n",
    "\n",
    "\n",
    "        for table_of_from in self.tables_of_from:\n",
    "            where_object = Where()\n",
    "            for i in range(0, len(column_offset)):\n",
    "                current = column_offset[i]\n",
    "\n",
    "                if i == 0:\n",
    "                    previous = 0\n",
    "                else:\n",
    "                    previous = column_offset[i - 1]\n",
    "\n",
    "                if i == (len(column_offset) - 1):\n",
    "                    _next = 999\n",
    "                else:\n",
    "                    _next = column_offset[i + 1]\n",
    "\n",
    "                junction = self.predict_junction(previous, current)\n",
    "                column = self.get_column_name_with_alias_table(columns_of_where[i], table_of_from)\n",
    "                operation_type = self.predict_operation_type(previous, current)\n",
    "\n",
    "                if len(self.columns_of_values_of_where) > i:\n",
    "                    value = self.columns_of_values_of_where[\n",
    "                        len(self.columns_of_values_of_where) - len(columns_of_where) + i]\n",
    "                else:\n",
    "                    value = 'OOV'  # Out Of Vocabulary: default value\n",
    "\n",
    "                operator = self.predict_operator(current, _next)\n",
    "                where_object.add_condition(junction, Condition(column, operation_type, operator, value))\n",
    "            self.where_objects.append(where_object)\n",
    "\n",
    "    def join(self):\n",
    "        Thread.join(self)\n",
    "        return self.where_objects\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Parser:\n",
    "    database_object = None\n",
    "    database_dico = None\n",
    "\n",
    "    count_keywords = []\n",
    "    sum_keywords = []\n",
    "    average_keywords = []\n",
    "    max_keywords = []\n",
    "    min_keywords = []\n",
    "    junction_keywords = []\n",
    "    disjunction_keywords = []\n",
    "    greater_keywords = []\n",
    "    less_keywords = []\n",
    "    between_keywords = []\n",
    "    order_by_keywords = []\n",
    "    asc_keywords = []\n",
    "    desc_keywords = []\n",
    "    group_by_keywords = []\n",
    "    negation_keywords = []\n",
    "    equal_keywords = []\n",
    "    like_keywords = []\n",
    "\n",
    "    def __init__(self, database, config):\n",
    "        self.database_object = database\n",
    "        self.database_dico = self.database_object.get_tables_into_dictionary()\n",
    "\n",
    "        self.count_keywords = config.get_count_keywords()\n",
    "        self.sum_keywords = config.get_sum_keywords()\n",
    "        self.average_keywords = config.get_avg_keywords()\n",
    "        self.max_keywords = config.get_max_keywords()\n",
    "        self.min_keywords = config.get_min_keywords()\n",
    "        self.junction_keywords = config.get_junction_keywords()\n",
    "        self.disjunction_keywords = config.get_disjunction_keywords()\n",
    "        self.greater_keywords = config.get_greater_keywords()\n",
    "        self.less_keywords = config.get_less_keywords()\n",
    "        self.between_keywords = config.get_between_keywords()\n",
    "        self.order_by_keywords = config.get_order_by_keywords()\n",
    "        self.asc_keywords = config.get_asc_keywords()\n",
    "        self.desc_keywords = config.get_desc_keywords()\n",
    "        self.group_by_keywords = config.get_group_by_keywords()\n",
    "        self.negation_keywords = config.get_negation_keywords()\n",
    "        self.equal_keywords = config.get_equal_keywords()\n",
    "        self.like_keywords = config.get_like_keywords()\n",
    "        self.distinct_keywords = config.get_distinct_keywords()\n",
    "\n",
    "    @staticmethod\n",
    "    def _myCmp(s1,s2):\n",
    "        if len(s1.split()) == len(s2.split()) :\n",
    "            if len(s1) >= len(s2) :\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            if len(s1.split()) >= len(s2.split()):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def transformation_sort(cls,transition_list):\n",
    "        # Sort on basis of two keys split length and then token lengths in the respective priority.\n",
    "        return sorted(transition_list, key=functools.cmp_to_key(cls._myCmp),reverse=True)\n",
    "\n",
    "\n",
    "    def remove_accents(self, string):\n",
    "        nkfd_form = unicodedata.normalize('NFKD', str(string))\n",
    "        return \"\".join([c for c in nkfd_form if not unicodedata.combining(c)])\n",
    "\n",
    "    def parse_sentence(self, sentence, stopwordsFilter=None):\n",
    "        sys.tracebacklimit = 0  # Remove traceback from Exception\n",
    "\n",
    "        number_of_table = 0\n",
    "        number_of_select_column = 0\n",
    "        number_of_where_column = 0\n",
    "        last_table_position = 0\n",
    "        columns_of_select = []\n",
    "        columns_of_where = []\n",
    "\n",
    "        if stopwordsFilter is not None:\n",
    "            sentence = stopwordsFilter.filter(sentence)\n",
    "\n",
    "        input_for_finding_value = sentence.rstrip(string.punctuation.replace('\"', '').replace(\"'\", \"\"))\n",
    "        columns_of_values_of_where = []\n",
    "\n",
    "        filter_list = [\",\", \"!\"]\n",
    "\n",
    "        for filter_element in filter_list:\n",
    "            input_for_finding_value = input_for_finding_value.replace(filter_element, \" \")\n",
    "\n",
    "        input_word_list = input_for_finding_value.split()\n",
    "\n",
    "        number_of_where_column_temp = 0\n",
    "        number_of_table_temp = 0\n",
    "        last_table_position_temp = 0\n",
    "        start_phrase = ''\n",
    "        med_phrase = ''\n",
    "\n",
    "        # TODO: merge this part of the algorithm (detection of values of where)\n",
    "        #  in the rest of the parsing algorithm (about line 725) '''\n",
    "\n",
    "        for i in range(0, len(input_word_list)):\n",
    "            for table_name in self.database_dico:\n",
    "                if (input_word_list[i] == table_name) or (\n",
    "                            input_word_list[i] in self.database_object.get_table_by_name(table_name).equivalences):\n",
    "                    if number_of_table_temp == 0:\n",
    "                        start_phrase = input_word_list[:i]\n",
    "                    number_of_table_temp += 1\n",
    "                    last_table_position_temp = i\n",
    "\n",
    "                columns = self.database_object.get_table_by_name(table_name).get_columns()\n",
    "                for column in columns:\n",
    "                    if (input_word_list[i] == column.name) or (input_word_list[i] in column.equivalences):\n",
    "                        if number_of_where_column_temp == 0:\n",
    "                            med_phrase = input_word_list[len(start_phrase):last_table_position_temp + 1]\n",
    "                        number_of_where_column_temp += 1\n",
    "                        break\n",
    "                    else:\n",
    "                        if (number_of_table_temp != 0) and (number_of_where_column_temp == 0) and (\n",
    "                                    i == (len(input_word_list) - 1)):\n",
    "                            med_phrase = input_word_list[len(start_phrase):]\n",
    "                else:\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "        end_phrase = input_word_list[len(start_phrase) + len(med_phrase):]\n",
    "\n",
    "        irext = ' '.join(end_phrase)\n",
    "\n",
    "        ''' @todo set this part of the algorithm (detection of values of where) in the WhereParser thread '''\n",
    "\n",
    "        if irext:\n",
    "            irext = self.remove_accents(irext.lower())\n",
    "\n",
    "            filter_list = [\",\", \"!\"]\n",
    "\n",
    "            for filter_element in filter_list:\n",
    "                irext = irext.replace(filter_element, \" \")\n",
    "\n",
    "            assignment_list = self.equal_keywords + self.like_keywords + self.greater_keywords + self.less_keywords + self.negation_keywords\n",
    "            # As these words can also be part of assigners\n",
    "\n",
    "            # custom operators added as they can be possibilities\n",
    "            assignment_list.append(':')\n",
    "            assignment_list.append('=')\n",
    "\n",
    "            # Algorithmic logic for best substitution for extraction of values with the help of assigners.\n",
    "            assignment_list = self.transformation_sort(assignment_list)\n",
    "\n",
    "            maverickjoy_general_assigner = \"*res*@3#>>*\"\n",
    "            maverickjoy_like_assigner = \"*like*@3#>>*\"\n",
    "\n",
    "            for idx, assigner in enumerate(assignment_list):\n",
    "                if assigner in self.like_keywords:\n",
    "                    assigner = str(\" \" + assigner + \" \")\n",
    "                    irext = irext.replace(assigner, str(\" \" + maverickjoy_like_assigner + \" \"))\n",
    "                else:\n",
    "                    assigner = str(\" \" + assigner + \" \")\n",
    "                    # Reason for adding \" \" these is according to the LOGIC implemented assigner operators help us extract the value,\n",
    "                    # hence they should be independent entities not part of some other big entity else logic will fail.\n",
    "                    # for eg -> \"show data for city where cityName where I like to risk my life  is Pune\" will end up extacting ,\n",
    "                    # 'k' and '1' both. I know its a lame sentence but something like this could be a problem.\n",
    "\n",
    "                    irext = irext.replace(assigner, str(\" \" + maverickjoy_general_assigner + \" \"))\n",
    "\n",
    "            # replace all spaces from values to <_> for proper value assignment in SQL\n",
    "            # eg. (where name is 'abc def') -> (where name is abc<_>def)\n",
    "            for i in re.findall(\"(['\\\"].*?['\\\"])\", irext):\n",
    "                irext = irext.replace(i, i.replace(' ', '<_>').replace(\"'\", '').replace('\"', ''))\n",
    "\n",
    "            irext_list = irext.split()\n",
    "\n",
    "            for idx, x in enumerate(irext_list):\n",
    "                index = idx + 1\n",
    "                if x == maverickjoy_like_assigner:\n",
    "                    if index < len(irext_list) and irext_list[index] != maverickjoy_like_assigner and irext_list[index] !=\\\n",
    "                            maverickjoy_general_assigner:\n",
    "                        # replace back <_> to spaces from the values assigned\n",
    "                        columns_of_values_of_where.append(str(\"'%\" + str(irext_list[index]).replace('<_>', ' ') + \"%'\"))\n",
    "\n",
    "                if x == maverickjoy_general_assigner:\n",
    "                    if index < len(irext_list) and irext_list[index] != maverickjoy_like_assigner and irext_list[index] != \\\n",
    "                            maverickjoy_general_assigner:\n",
    "                        # replace back <_> to spaces from the values assigned\n",
    "                        columns_of_values_of_where.append(str(\"'\" + str(irext_list[index]).replace('<_>', ' ') + \"'\"))\n",
    "\n",
    "        ''' ----------------------------------------------------------------------------------------------------------- '''\n",
    "\n",
    "        tables_of_from = []\n",
    "        select_phrase = ''\n",
    "        from_phrase = ''\n",
    "        where_phrase = ''\n",
    "\n",
    "        words = re.findall(r\"[\\w]+\", self.remove_accents(sentence))\n",
    "\n",
    "        for i in range(0, len(words)):\n",
    "            for table_name in self.database_dico:\n",
    "                if (words[i] == table_name) or (\n",
    "                            words[i] in self.database_object.get_table_by_name(table_name).equivalences):\n",
    "                    if number_of_table == 0:\n",
    "                        select_phrase = words[:i]\n",
    "                    tables_of_from.append(table_name)\n",
    "                    number_of_table += 1\n",
    "                    last_table_position = i\n",
    "\n",
    "                columns = self.database_object.get_table_by_name(table_name).get_columns()\n",
    "                for column in columns:\n",
    "                    if (words[i] == column.name) or (words[i] in column.equivalences):\n",
    "                        if number_of_table == 0:\n",
    "                            columns_of_select.append(column.name)\n",
    "                            number_of_select_column += 1\n",
    "                        else:\n",
    "                            if number_of_where_column == 0:\n",
    "                                from_phrase = words[len(select_phrase):last_table_position + 1]\n",
    "                            columns_of_where.append(column.name)\n",
    "                            number_of_where_column += 1\n",
    "                        break\n",
    "                    else:\n",
    "                        if (number_of_table != 0) and (number_of_where_column == 0) and (i == (len(words) - 1)):\n",
    "                            from_phrase = words[len(select_phrase):]\n",
    "\n",
    "        where_phrase = words[len(select_phrase) + len(from_phrase):]\n",
    "\n",
    "        if (number_of_select_column + number_of_table + number_of_where_column) == 0:\n",
    "            raise ParsingException(\"No keyword found in sentence!\")\n",
    "\n",
    "        if len(tables_of_from) > 0:\n",
    "            from_phrases = []\n",
    "            previous_index = 0\n",
    "            for i in range(0, len(from_phrase)):\n",
    "                for table in tables_of_from:\n",
    "                    if (from_phrase[i] == table) or (\n",
    "                                from_phrase[i] in self.database_object.get_table_by_name(table).equivalences):\n",
    "                        from_phrases.append(from_phrase[previous_index:i + 1])\n",
    "                        previous_index = i + 1\n",
    "\n",
    "            last_junction_word_index = -1\n",
    "\n",
    "            for i in range(0, len(from_phrases)):\n",
    "                number_of_junction_words = 0\n",
    "                number_of_disjunction_words = 0\n",
    "\n",
    "                for word in from_phrases[i]:\n",
    "                    if word in self.junction_keywords:\n",
    "                        number_of_junction_words += 1\n",
    "                    if word in self.disjunction_keywords:\n",
    "                        number_of_disjunction_words += 1\n",
    "\n",
    "                if (number_of_junction_words + number_of_disjunction_words) > 0:\n",
    "                    last_junction_word_index = i\n",
    "\n",
    "            if last_junction_word_index == -1:\n",
    "                from_phrase = sum(from_phrases[:1], [])\n",
    "                where_phrase = sum(from_phrases[1:], []) + where_phrase\n",
    "            else:\n",
    "                from_phrase = sum(from_phrases[:last_junction_word_index + 1], [])\n",
    "                where_phrase = sum(from_phrases[last_junction_word_index + 1:], []) + where_phrase\n",
    "\n",
    "        real_tables_of_from = []\n",
    "\n",
    "        for word in from_phrase:\n",
    "            for table in tables_of_from:\n",
    "                if (word == table) or (word in self.database_object.get_table_by_name(table).equivalences):\n",
    "                    real_tables_of_from.append(table)\n",
    "\n",
    "        tables_of_from = real_tables_of_from\n",
    "\n",
    "        if len(tables_of_from) == 0:\n",
    "            raise ParsingException(\"No table name found in sentence!\")\n",
    "\n",
    "        group_by_phrase = []\n",
    "        order_by_phrase = []\n",
    "        new_where_phrase = []\n",
    "        previous_index = 0\n",
    "        previous_phrase_type = 0\n",
    "        yet_where = 0\n",
    "\n",
    "        for i in range(0, len(where_phrase)):\n",
    "            if where_phrase[i] in self.order_by_keywords:\n",
    "                if yet_where > 0:\n",
    "                    if previous_phrase_type == 1:\n",
    "                        order_by_phrase.append(where_phrase[previous_index:i])\n",
    "                    elif previous_phrase_type == 2:\n",
    "                        group_by_phrase.append(where_phrase[previous_index:i])\n",
    "                else:\n",
    "                    new_where_phrase.append(where_phrase[previous_index:i])\n",
    "                previous_index = i\n",
    "                previous_phrase_type = 1\n",
    "                yet_where += 1\n",
    "            if where_phrase[i] in self.group_by_keywords:\n",
    "                if yet_where > 0:\n",
    "                    if previous_phrase_type == 1:\n",
    "                        order_by_phrase.append(where_phrase[previous_index:i])\n",
    "                    elif previous_phrase_type == 2:\n",
    "                        group_by_phrase.append(where_phrase[previous_index:i])\n",
    "                else:\n",
    "                    new_where_phrase.append(where_phrase[previous_index:i])\n",
    "                previous_index = i\n",
    "                previous_phrase_type = 2\n",
    "                yet_where += 1\n",
    "\n",
    "        if previous_phrase_type == 1:\n",
    "            order_by_phrase.append(where_phrase[previous_index:])\n",
    "        elif previous_phrase_type == 2:\n",
    "            group_by_phrase.append(where_phrase[previous_index:])\n",
    "        else:\n",
    "            print(where_phrase)\n",
    "            new_where_phrase.append(where_phrase)\n",
    "\n",
    "        try:\n",
    "            select_parser = SelectParser(columns_of_select, tables_of_from, select_phrase, self.count_keywords,\n",
    "                                         self.sum_keywords, self.average_keywords, self.max_keywords, self.min_keywords,\n",
    "                                         self.distinct_keywords, self.database_dico, self.database_object)\n",
    "            from_parser = FromParser(tables_of_from, columns_of_select, columns_of_where, self.database_object)\n",
    "            where_parser = WhereParser(new_where_phrase, tables_of_from, columns_of_values_of_where,\n",
    "                                       self.count_keywords, self.sum_keywords, self.average_keywords, self.max_keywords,\n",
    "                                       self.min_keywords, self.greater_keywords, self.less_keywords,\n",
    "                                       self.between_keywords, self.negation_keywords, self.junction_keywords,\n",
    "                                       self.disjunction_keywords, self.like_keywords, self.distinct_keywords,\n",
    "                                       self.database_dico, self.database_object)\n",
    "            group_by_parser = GroupByParser(group_by_phrase, tables_of_from, self.database_dico, self.database_object)\n",
    "            order_by_parser = OrderByParser(order_by_phrase, tables_of_from, self.asc_keywords, self.desc_keywords,\n",
    "                                            self.database_dico, self.database_object)\n",
    "\n",
    "            select_parser.start()\n",
    "            from_parser.start()\n",
    "            where_parser.start()\n",
    "            group_by_parser.start()\n",
    "            order_by_parser.start()\n",
    "\n",
    "            queries = from_parser.join()\n",
    "        except:\n",
    "            raise ParsingException(\"Parsing error occured in thread!\")\n",
    "\n",
    "        if isinstance(queries, ParsingException):\n",
    "            raise queries\n",
    "\n",
    "        try:\n",
    "            select_objects = select_parser.join()\n",
    "            where_objects = where_parser.join()\n",
    "            group_by_objects = group_by_parser.join()\n",
    "            order_by_objects = order_by_parser.join()\n",
    "        except:\n",
    "            raise ParsingException(\"Parsing error occured in thread!\")\n",
    "\n",
    "        for i in range(0, len(queries)):\n",
    "            query = queries[i]\n",
    "            query.set_select(select_objects[i])\n",
    "            query.set_where(where_objects[i])\n",
    "            query.set_group_by(group_by_objects[i])\n",
    "            query.set_order_by(order_by_objects[i])\n",
    "\n",
    "        return queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Thesaurus:\n",
    "    def __init__(self):\n",
    "        self.dictionary = {}\n",
    "\n",
    "    def add_entry(self, word, synonyms):\n",
    "        self.dictionary[word] = synonyms\n",
    "\n",
    "    def add_synonym_to_a_word(self, word, synonym):\n",
    "        self.dictionary[word].append(synonym)\n",
    "\n",
    "    def add_synonyms_to_a_word(self, word, synonyms):\n",
    "        if word in self.dictionary:\n",
    "            self.dictionary[word] += synonyms\n",
    "        else:\n",
    "            self.dictionary[word] = synonyms\n",
    "\n",
    "    def get_synonyms_of_a_word(self, word):\n",
    "        if word in list(self.dictionary.keys()):\n",
    "            return self.dictionary[word]\n",
    "\n",
    "    def remove_accents(self, string):\n",
    "        nkfd_form = unicodedata.normalize('NFKD', str(string))\n",
    "        return \"\".join([c for c in nkfd_form if not unicodedata.combining(c)])\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_path(path):\n",
    "        cwd = os.path.dirname(__file__)\n",
    "        filename = os.path.join(cwd, path)\n",
    "        return filename\n",
    "\n",
    "    def load(self, path):\n",
    "        with open(self._generate_path(path)) as f:\n",
    "            content = f.readlines()\n",
    "            # we jump content[0] because it is the encoding-type line : useless to parse\n",
    "            for line_id in range(1, len(content)):\n",
    "                if '(' not in content[line_id]:\n",
    "                    line = content[line_id].split(\"|\")\n",
    "                    word = self.remove_accents(line[0])\n",
    "                    synonyms = self.remove_accents(content[line_id + 1]).split(\"|\")\n",
    "                    synonyms.pop(0)\n",
    "                    self.add_synonyms_to_a_word(word, synonyms)\n",
    "\n",
    "    def print_me(self):\n",
    "        for keys, values in list(self.dictionary.items()):\n",
    "            print(keys)\n",
    "            print(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "class StopwordFilter:\n",
    "    def __init__(self):\n",
    "        self.list = []\n",
    "\n",
    "    def add_stopword(self, word):\n",
    "        self.list.append(word)\n",
    "\n",
    "    def get_stopword_list(self):\n",
    "        return self.list\n",
    "\n",
    "    def filter(self, sentence):\n",
    "        tmp_sentence = \"\"\n",
    "        words = re.findall(r\"[\\w]+\", self.remove_accents(sentence))\n",
    "        for word in words:\n",
    "            word = self.remove_accents(word).lower()\n",
    "            if word not in self.list:\n",
    "                tmp_sentence += word + \" \"\n",
    "        return tmp_sentence.strip()\n",
    "\n",
    "    def remove_accents(self, string):\n",
    "        nkfd_form = unicodedata.normalize('NFKD', str(string))\n",
    "        return \"\".join([c for c in nkfd_form if not unicodedata.combining(c)])\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_path(path):\n",
    "        cwd = os.path.dirname(__file__)\n",
    "        filename = os.path.join(cwd, path)\n",
    "        return filename\n",
    "\n",
    "    def load(self, path):\n",
    "        with open(self._generate_path(path)) as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            for word in lines:\n",
    "                stopword = self.remove_accents(word).lower()\n",
    "                self.add_stopword(stopword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "class LangConfig:\n",
    "    def __init__(self):\n",
    "        self.avg_keywords = []\n",
    "        self.sum_keywords = []\n",
    "        self.max_keywords = []\n",
    "        self.min_keywords = []\n",
    "        self.count_keywords = []\n",
    "        self.junction_keywords = []\n",
    "        self.disjunction_keywords = []\n",
    "        self.greater_keywords = []\n",
    "        self.less_keywords = []\n",
    "        self.between_keywords = []\n",
    "        self.order_by_keywords = []\n",
    "        self.asc_keywords = []\n",
    "        self.desc_keywords = []\n",
    "        self.group_by_keywords = []\n",
    "        self.negation_keywords = []\n",
    "        self.equal_keywords = []\n",
    "        self.like_keywords = []\n",
    "        self.distinct_keywords = []\n",
    "\n",
    "    def get_avg_keywords(self):\n",
    "        return self.avg_keywords\n",
    "\n",
    "    def get_sum_keywords(self):\n",
    "        return self.sum_keywords\n",
    "\n",
    "    def get_max_keywords(self):\n",
    "        return self.max_keywords\n",
    "\n",
    "    def get_min_keywords(self):\n",
    "        return self.min_keywords\n",
    "\n",
    "    def get_count_keywords(self):\n",
    "        return self.count_keywords\n",
    "\n",
    "    def get_junction_keywords(self):\n",
    "        return self.junction_keywords\n",
    "\n",
    "    def get_disjunction_keywords(self):\n",
    "        return self.disjunction_keywords\n",
    "\n",
    "    def get_greater_keywords(self):\n",
    "        return self.greater_keywords\n",
    "\n",
    "    def get_less_keywords(self):\n",
    "        return self.less_keywords\n",
    "\n",
    "    def get_between_keywords(self):\n",
    "        return self.between_keywords\n",
    "\n",
    "    def get_order_by_keywords(self):\n",
    "        return self.order_by_keywords\n",
    "\n",
    "    def get_asc_keywords(self):\n",
    "        return self.asc_keywords\n",
    "\n",
    "    def get_desc_keywords(self):\n",
    "        return self.desc_keywords\n",
    "\n",
    "    def get_group_by_keywords(self):\n",
    "        return self.group_by_keywords\n",
    "\n",
    "    def get_negation_keywords(self):\n",
    "        return self.negation_keywords\n",
    "\n",
    "    def get_equal_keywords(self):\n",
    "        return self.equal_keywords\n",
    "\n",
    "    def get_like_keywords(self):\n",
    "        return self.like_keywords\n",
    "\n",
    "    def get_distinct_keywords(self):\n",
    "        return self.distinct_keywords\n",
    "\n",
    "    def remove_accents(self, string):\n",
    "        nkfd_form = unicodedata.normalize('NFKD', str(string))\n",
    "        return \"\".join([c for c in nkfd_form if not unicodedata.combining(c)])\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_path(path):\n",
    "        cwd = os.path.dirname(__file__)\n",
    "        filename = os.path.join(cwd, path)\n",
    "        return filename\n",
    "\n",
    "    def load(self, path):\n",
    "        with open(path) as f:\n",
    "            content = f.readlines()\n",
    "            self.avg_keywords = list(\n",
    "                map(self.remove_accents, list(map(str.strip, content[0].replace(':', ',').split(\",\")))))\n",
    "            self.avg_keywords = self.avg_keywords[1:len(self.avg_keywords)]\n",
    "            self.avg_keywords = [keyword.lower() for keyword in self.avg_keywords]\n",
    "\n",
    "            self.sum_keywords = list(\n",
    "                map(self.remove_accents, list(map(str.strip, content[1].replace(':', ',').split(\",\")))))\n",
    "            self.sum_keywords = self.sum_keywords[1:len(self.sum_keywords)]\n",
    "            self.sum_keywords = [keyword.lower() for keyword in self.sum_keywords]\n",
    "\n",
    "            self.max_keywords = list(\n",
    "                map(self.remove_accents, list(map(str.strip, content[2].replace(':', ',').split(\",\")))))\n",
    "            self.max_keywords = self.max_keywords[1:len(self.max_keywords)]\n",
    "            self.max_keywords = [keyword.lower() for keyword in self.max_keywords]\n",
    "\n",
    "            self.min_keywords = list(\n",
    "                map(self.remove_accents, list(map(str.strip, content[3].replace(':', ',').split(\",\")))))\n",
    "            self.min_keywords = self.min_keywords[1:len(self.min_keywords)]\n",
    "            self.min_keywords = [keyword.lower() for keyword in self.min_keywords]\n",
    "\n",
    "            self.count_keywords = list(\n",
    "                map(self.remove_accents, list(map(str.strip, content[4].replace(':', ',').split(\",\")))))\n",
    "            self.count_keywords = self.count_keywords[1:len(self.count_keywords)]\n",
    "            self.count_keywords = [keyword.lower() for keyword in self.count_keywords]\n",
    "\n",
    "            self.junction_keywords = list(\n",
    "                map(self.remove_accents, list(map(str.strip, content[5].replace(':', ',').split(\",\")))))\n",
    "            self.junction_keywords = self.junction_keywords[1:len(self.junction_keywords)]\n",
    "            self.junction_keywords = [keyword.lower() for keyword in self.junction_keywords]\n",
    "\n",
    "            self.disjunction_keywords = list(\n",
    "                map(self.remove_accents, list(map(str.strip, content[6].replace(':', ',').split(\",\")))))\n",
    "            self.disjunction_keywords = self.disjunction_keywords[1:len(self.disjunction_keywords)]\n",
    "            self.disjunction_keywords = [keyword.lower() for keyword in self.disjunction_keywords]\n",
    "\n",
    "            self.greater_keywords = list(\n",
    "                map(self.remove_accents, list(map(str.strip, content[7].replace(':', ',').split(\",\")))))\n",
    "            self.greater_keywords = self.greater_keywords[1:len(self.greater_keywords)]\n",
    "            self.greater_keywords = [keyword.lower() for keyword in self.greater_keywords]\n",
    "\n",
    "            self.less_keywords = list(\n",
    "                map(self.remove_accents, list(map(str.strip, content[8].replace(':', ',').split(\",\")))))\n",
    "            self.less_keywords = self.less_keywords[1:len(self.less_keywords)]\n",
    "            self.less_keywords = [keyword.lower() for keyword in self.less_keywords]\n",
    "\n",
    "            self.between_keywords = list(\n",
    "                map(self.remove_accents, list(map(str.strip, content[9].replace(':', ',').split(\",\")))))\n",
    "            self.between_keywords = self.between_keywords[1:len(self.between_keywords)]\n",
    "            self.between_keywords = [keyword.lower() for keyword in self.between_keywords]\n",
    "\n",
    "            self.order_by_keywords = list(\n",
    "                map(self.remove_accents, list(map(str.strip, content[10].replace(':', ',').split(\",\")))))\n",
    "            self.order_by_keywords = self.order_by_keywords[1:len(self.order_by_keywords)]\n",
    "            self.order_by_keywords = [keyword.lower() for keyword in self.order_by_keywords]\n",
    "\n",
    "            self.asc_keywords = list(\n",
    "                map(self.remove_accents, list(map(str.strip, content[11].replace(':', ',').split(\",\")))))\n",
    "            self.asc_keywords = self.asc_keywords[1:len(self.asc_keywords)]\n",
    "            self.asc_keywords = [keyword.lower() for keyword in self.asc_keywords]\n",
    "\n",
    "            self.desc_keywords = list(\n",
    "                map(self.remove_accents, list(map(str.strip, content[12].replace(':', ',').split(\",\")))))\n",
    "            self.desc_keywords = self.desc_keywords[1:len(self.desc_keywords)]\n",
    "            self.desc_keywords = [keyword.lower() for keyword in self.desc_keywords]\n",
    "\n",
    "            self.negation_keywords = list(\n",
    "                map(self.remove_accents, list(map(str.strip, content[14].replace(':', ',').split(\",\")))))\n",
    "            self.negation_keywords = self.negation_keywords[1:len(self.negation_keywords)]\n",
    "            self.negation_keywords = [keyword.lower() for keyword in self.negation_keywords]\n",
    "\n",
    "            self.equal_keywords = list(\n",
    "                map(self.remove_accents, list(map(str.strip, content[15].replace(':', ',').split(\",\")))))\n",
    "            self.equal_keywords = self.equal_keywords[1:len(self.equal_keywords)]\n",
    "            self.equal_keywords = [keyword.lower() for keyword in self.equal_keywords]\n",
    "\n",
    "            self.like_keywords = list(\n",
    "                map(self.remove_accents, list(map(str.strip, content[16].replace(':', ',').split(\",\")))))\n",
    "            self.like_keywords = self.like_keywords[1:len(self.like_keywords)]\n",
    "            self.like_keywords = [keyword.lower() for keyword in self.like_keywords]\n",
    "\n",
    "            self.distinct_keywords = list(\n",
    "                map(self.remove_accents, list(map(str.strip, content[17].replace(':', ',').split(\",\")))))\n",
    "            self.distinct_keywords = self.distinct_keywords[1:len(self.distinct_keywords)]\n",
    "            self.distinct_keywords = [keyword.lower() for keyword in self.distinct_keywords]\n",
    "\n",
    "    def print_me(self):\n",
    "        print(self.avg_keywords)\n",
    "        print(self.sum_keywords)\n",
    "        print(self.max_keywords)\n",
    "        print(self.min_keywords)\n",
    "        print(self.count_keywords)\n",
    "        print(self.junction_keywords)\n",
    "        print(self.disjunction_keywords)\n",
    "        print(self.greater_keywords)\n",
    "        print(self.less_keywords)\n",
    "        print(self.between_keywords)\n",
    "        print(self.order_by_keywords)\n",
    "        print(self.asc_keywords)\n",
    "        print(self.desc_keywords)\n",
    "        print(self.group_by_keywords)\n",
    "        print(self.negation_keywords)\n",
    "        print(self.equal_keywords)\n",
    "        print(self.like_keywords)\n",
    "        print(self.distinct_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import argparse\n",
    "import os\n",
    "class Ln2sql:\n",
    "    def __init__(\n",
    "            self,\n",
    "            database_path,\n",
    "            language_path,\n",
    "            json_output_path=None,\n",
    "            thesaurus_path=None,\n",
    "            stopwords_path=None,\n",
    "            color=False\n",
    "    ):\n",
    "        if color == False:\n",
    "            without_color()\n",
    "\n",
    "        database = Database()\n",
    "        self.stopwordsFilter = None\n",
    "\n",
    "        if thesaurus_path:\n",
    "            thesaurus = Thesaurus()\n",
    "            thesaurus.load(thesaurus_path)\n",
    "            database.set_thesaurus(thesaurus)\n",
    "\n",
    "        if stopwords_path:\n",
    "            self.stopwordsFilter = StopwordFilter()\n",
    "            self.stopwordsFilter.load(stopwords_path)\n",
    "\n",
    "        database.load(database_path)\n",
    "        database.print_me()\n",
    "\n",
    "        config = LangConfig()\n",
    "        config.load(language_path)\n",
    "\n",
    "        self.parser = Parser(database, config)\n",
    "        self.json_output_path = json_output_path\n",
    "        \n",
    "\n",
    "    def get_query(self, input_sentence):\n",
    "        queries = self.parser.parse_sentence(input_sentence, self.stopwordsFilter)\n",
    "\n",
    "        if self.json_output_path:\n",
    "            self.remove_json(self.json_output_path)\n",
    "            for query in queries:\n",
    "                query.print_json(self.json_output_path)\n",
    "\n",
    "        full_query = ''\n",
    "\n",
    "        for query in queries:\n",
    "            full_query += str(query)\n",
    "            print(query)\n",
    "\n",
    "        return full_query\n",
    "\n",
    "    def remove_json(self, filename=\"output.json\"):\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|                      CITY           |\n",
      "+-------------------------------------+\n",
      "| ðŸ”‘                        id (int)           |\n",
      "|         cityName (string)           |\n",
      "+-------------------------------------+\n",
      "\n",
      "+-------------------------------------+\n",
      "|                       EMP           |\n",
      "+-------------------------------------+\n",
      "| ðŸ”‘                        id (int)           |\n",
      "|             name (string)           |\n",
      "| #ï¸âƒ£                    cityId (int)           |\n",
      "|               score (int)           |\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 102\n",
      "+-------------------------------------+\n",
      "\n",
      "[]\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'GroupByParser' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "ParsingException: Parsing error occured in thread!\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AttributeError: 'ParsingException' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AssertionError\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'GroupByParser' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "ParsingException: Parsing error occured in thread!\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AttributeError: 'ParsingException' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "TypeError: can only concatenate str (not \"list\") to str\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AssertionError\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'GroupByParser' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "ParsingException: Parsing error occured in thread!\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AttributeError: 'ParsingException' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "TypeError: can only concatenate str (not \"list\") to str\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "TypeError: can only concatenate str (not \"list\") to str\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "ln2sql = Ln2sql('database_store/city.sql','lang_store/english.csv')\n",
    "ln2sql.get_query('what is the id of the city ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
